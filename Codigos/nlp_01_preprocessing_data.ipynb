{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deathperminut/NLP_EstocasticProcess/blob/main/nlp_01_preprocessing_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "6248052f-e022-42fb-aa0f-6ccc102d3d67",
        "_uuid": "75c9b083f58514b6eb09c993027b6f1ffb135272",
        "id": "jkTuECJr2H1n"
      },
      "source": [
        "# PREPROCESSING DATA FOR NLP\n",
        "\n",
        "Three steps : \n",
        "* Tokens \n",
        "* Normalization : remove blanks, stopwords,... stemming and lemmatization\n",
        "* Removing noise : remove HTML tag for example\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LIBRERIAS"
      ],
      "metadata": {
        "id": "NSRFZQf18IGy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "_cell_guid": "021f4307-9c60-4e64-a0de-81c7817e8e23",
        "_uuid": "25aa7372b54e440fe1e0a2cff5b8ade54a3cd1bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ad_MF2s92H14",
        "outputId": "572d35a1-94e8-4337-bb94-6745af8d7db2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from subprocess import check_output\n",
        "\n",
        "\n",
        "import re\n",
        "import string\n",
        "import numpy as np\n",
        "import nltk\n",
        "from collections import Counter\n",
        "from nltk import tokenize\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "\n",
        "import nltk.data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unidecode\n",
        "import unidecode"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pYGtBKo98rV",
        "outputId": "772b1ec4-3f13-4934-e71b-2e01f96e432b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.6-py3-none-any.whl (235 kB)\n",
            "\u001b[K     |████████████████████████████████| 235 kB 5.0 MB/s \n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.3.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "9aab05f1-d07e-4b80-bd55-d8b6a0cd3249",
        "_uuid": "a7f5c813cc1c76f9253ab94a44d6d5902c549033",
        "id": "x4P2BnE52H2E"
      },
      "source": [
        "# TOKENIZATION (function examples)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "_cell_guid": "8fc5dfa1-b15b-4db7-8e95-a32d27c4c7c9",
        "_uuid": "37b99cf01360c3b1482511421a044e868ddd2bfd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRY4n1cr2H2I",
        "outputId": "cbd44ffb-50a0-4e5d-a438-17e5dfcca7a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ThIs's   ã sent tokenize test  .  this's sent two. is this sent three? sent 4 is cool! Now it's your turn.\n"
          ]
        }
      ],
      "source": [
        "text1 = \"ThIs's   ã sent tokenize test  .  this's sent two. is this sent three? sent 4 is cool! Now it's your turn.\"\n",
        "print(text1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3HmLAf32H2N"
      },
      "source": [
        "## Words Tokenization\n",
        "\n",
        "** Recall : ** sent_tokenize is one of instances of PunktSentenceTokenizer from the nltk.tokenize.punkt module. \n",
        "\n",
        "* Naive split\n",
        "* Use of \"word_tokenize\" function : Actually, word_tokenize is a wrapper function that calls tokenize by the TreebankWordTokenizer\n",
        "* Use of \"PunktWordTokenizer\" : splits on punctuation, but keeps it with the word\n",
        "* use of \"WordPunktTokenizer\" : splits all punctuations into separate tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FnlNNu712H2P",
        "outputId": "155670d1-4a0a-47dd-fa71-61e52299837d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "With a naive split \n",
            " [\"ThIs's\", '', '', 'ã', 'sent', 'tokenize', 'test', '', '.', '', \"this's\", 'sent', 'two.', 'is', 'this', 'sent', 'three?', 'sent', '4', 'is', 'cool!', 'Now', \"it's\", 'your', 'turn.']\n",
            "\n",
            "Tokenizing text into words With NLTK \n",
            " ['ThIs', \"'s\", 'ã', 'sent', 'tokenize', 'test', '.', 'this', \"'s\", 'sent', 'two', '.', 'is', 'this', 'sent', 'three', '?', 'sent', '4', 'is', 'cool', '!', 'Now', 'it', \"'s\", 'your', 'turn', '.']\n",
            "\n",
            "Equivalent method with TreebankWordTokenizer \n",
            " ['ThIs', \"'s\", 'ã', 'sent', 'tokenize', 'test', '.', 'this', \"'s\", 'sent', 'two.', 'is', 'this', 'sent', 'three', '?', 'sent', '4', 'is', 'cool', '!', 'Now', 'it', \"'s\", 'your', 'turn', '.']\n",
            "\n",
            "Equivalent method with WordPunctTokenizer \n",
            " ['ThIs', \"'\", 's', 'ã', 'sent', 'tokenize', 'test', '.', 'this', \"'\", 's', 'sent', 'two', '.', 'is', 'this', 'sent', 'three', '?', 'sent', '4', 'is', 'cool', '!', 'Now', 'it', \"'\", 's', 'your', 'turn', '.']\n"
          ]
        }
      ],
      "source": [
        "\"\"\" \n",
        "Split\n",
        "\"\"\"\n",
        "print(\"With a naive split \\n\", text1.split(\" \"))## ENTREGA EN UNA LISTA TODAS LAS PALABRAS SEPARADAS POR ESPACIO\n",
        "\n",
        "\"\"\"\n",
        "Tokenizing text into words\n",
        "\"\"\"\n",
        "tokens = nltk.word_tokenize(text1)\n",
        "print(\"\\nTokenizing text into words With NLTK \\n\", tokens)\n",
        "\n",
        "\"\"\"\n",
        "Equivalent method with TreebankWordTokenizer\n",
        "\"\"\"\n",
        "tokenizer = TreebankWordTokenizer()\n",
        "print(\"\\nEquivalent method with TreebankWordTokenizer \\n\", tokenizer.tokenize(text1))\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Equivalent method with WordPunctTokenizer  \n",
        "\"\"\"\n",
        "from nltk.tokenize import WordPunctTokenizer \n",
        "tokenizer = WordPunctTokenizer()\n",
        "print(\"\\nEquivalent method with WordPunctTokenizer \\n\", tokenizer.tokenize(text1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "collapsed": true,
        "id": "gfLmoOR_2H2U"
      },
      "outputs": [],
      "source": [
        "def tokenize_word_text(text): \n",
        "    tokens = nltk.word_tokenize(text) \n",
        "    tokens = [token.strip() for token in tokens] \n",
        "    return tokens "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "91bde106-f36f-4e7d-8f32-e8a2fceb62dd",
        "_uuid": "f51f146b4cb7381425ab04ac2f293dda9f309bc1",
        "id": "1x8HIZQ-2H2Y"
      },
      "source": [
        "##  Sentence tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "_cell_guid": "10a6b6fd-ca0f-469f-b99d-18a052442437",
        "_uuid": "a5e66b0817e7ce8e0374884ddd8590d795463f37",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gMKkWSgv2H2a",
        "outputId": "28a4593a-bf99-4e7b-f82a-7c224d39bb9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence tokenize in NLTK With sent_tokenize \n",
            " [\"ThIs's   ã sent tokenize test  .\", \"this's sent two.\", 'is this sent three?', 'sent 4 is cool!', \"Now it's your turn.\"]\n",
            "[\"ThIs's   ã sent tokenize test  .\", \"this's sent two.\", 'is this sent three?', 'sent 4 is cool!', \"Now it's your turn.\"]\n",
            "\n",
            "Sentence tokenize with PunktSentenceTokenizer \n",
            "  None\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Sentence tokenize in NLTK with sent_tokenize \n",
        "The sent_tokenize function uses an instance of NLTK known as PunktSentenceTokenizer\n",
        "This instance of NLTK has already been trained to perform tokenization on different European languages on the basis of letters or punctuation that mark the beginning and end of sentences\n",
        "\"\"\"\n",
        "from nltk.tokenize import sent_tokenize\n",
        "print(\"Sentence tokenize in NLTK With sent_tokenize \\n\", sent_tokenize(text1))\n",
        "\n",
        "\"\"\"\n",
        "Autres manières \n",
        "\"\"\"\n",
        "## using PunktSentenceTokenizer for sentence tokenization \n",
        "punkt_st = nltk.tokenize.PunktSentenceTokenizer() \n",
        "sample_sentences = punkt_st.tokenize(text1) \n",
        "print(\"\\nSentence tokenize with PunktSentenceTokenizer \\n \", print(sample_sentences) )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "6fa92b7c-5d00-4d0e-adb0-7a6801543378",
        "_uuid": "ed973018163557dc78e7f5efcd4b808a770034d7",
        "id": "hULEzV542H2i"
      },
      "source": [
        "## Probando otros lenguajes \n",
        "\n",
        "There are total 17 european languages that NLTK support for sentence tokenize, and you can use them as the following steps:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "_cell_guid": "ec408ea0-0b3d-4db2-ba2b-e516908d437b",
        "_uuid": "e944b950900034d4e2ac8f68ac8637062705ef79",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0IOXxdj2H2k",
        "outputId": "3bb5ebd5-9f19-4a4a-85bf-ad232e6262e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " ['Die Orgellandschaft Südniedersachsen umfasst das Gebiet der Landkreise Goslar, Göttingen, Hameln-Pyrmont, Hildesheim, Holzminden, Northeim und Osterode am Harz sowie die Stadt Salzgitter.', 'Über 70 historische Orgeln vom 17. bis 19. Jahrhundert sind in der südniedersächsischen Orgellandschaft vollständig oder in Teilen erhalten.']\n",
            "\n",
            " ['Die Orgellandschaft Südniedersachsen umfasst das Gebiet der Landkreise Goslar, Göttingen, Hameln-Pyrmont, Hildesheim, Holzminden, Northeim und Osterode am Harz sowie die Stadt Salzgitter.', 'Über 70 historische Orgeln vom 17. bis 19.', 'Jahrhundert sind in der südniedersächsischen Orgellandschaft vollständig oder in Teilen erhalten.']\n",
            "English token  [\"ThIs's   ã sent tokenize test  .\", \"this's sent two.\", 'is this sent three?', 'sent 4 is cool!', \"Now it's your turn.\"]\n",
            "\n",
            "French token  [\"Il fait beau aujourd'hui.\", 'Vas-tu sortir ?', \"N'y a-t-il pas du pain ?\"]\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Two ways to proceed:\n",
        "\"\"\"\n",
        "\n",
        "\"\"\" \n",
        "Method 1\n",
        "\"\"\"\n",
        "german_text = u\"Die Orgellandschaft Südniedersachsen umfasst das Gebiet der Landkreise Goslar, Göttingen, Hameln-Pyrmont, Hildesheim, Holzminden, Northeim und Osterode am Harz sowie die Stadt Salzgitter. Über 70 historische Orgeln vom 17. bis 19. Jahrhundert sind in der südniedersächsischen Orgellandschaft vollständig oder in Teilen erhalten. \"\n",
        "print(\"\\n\",sent_tokenize(german_text, language='german'))\n",
        "print(\"\\n\",sent_tokenize(german_text, language='polish'))\n",
        "\n",
        "\"\"\" \n",
        "Method 2\n",
        "\"\"\"\n",
        "\n",
        "# English\n",
        "tokenizer = nltk.data.load(\"tokenizers/punkt/english.pickle\")\n",
        "print(\"English token \", tokenizer.tokenize(text1))\n",
        "\n",
        "# French\n",
        "french_tokenizer = nltk.data.load(\"tokenizers/punkt/french.pickle\")\n",
        "print(\"\\nFrench token \", french_tokenizer.tokenize(\"Il fait beau aujourd'hui. Vas-tu sortir ? N'y a-t-il pas du pain ?\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "d4a28a3d-c1d3-4391-853b-34b983a986e4",
        "_uuid": "837af5607cb061b0f94c5eaf500ef850cf71e80e",
        "id": "3EaohPm02H2r"
      },
      "source": [
        "# TOKENIZATION PREPROCESO\n",
        "\n",
        "## I/ PRIMEROS PASOS\n",
        "\n",
        "* Convertimos todas las palabras en minusculas o mayusculas\n",
        "* Eliminamos espacios en blanco\n",
        "* Eliminamos signos de puntuación y caracteres especiales, este paso generalmente se realiza antes de generar los tokens\n",
        "* Convertimos numeros en palabras\n",
        "* eliminamos tildes y marcas de acento.\n",
        "* Expandimos abreviaciones\n",
        "* Eliminamos stopwords o palabras muy comunes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#GENERAMOS LOS TOKENS"
      ],
      "metadata": {
        "id": "Nub_rEpl6xMb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "_cell_guid": "a27241c7-f2ff-4690-a5b4-e7a6c4c80763",
        "_uuid": "31d5f7a70ff38cdd77a5a457805672d2e0734599",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o89kRomS2H2t",
        "outputId": "adec3778-47c0-43a3-abf9-5220ea2dcdb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ThIs', \"'s\", 'ã', 'sent', 'tokenize', 'test', '.', 'this', \"'s\", 'sent', 'two', '.', 'is', 'this', 'sent', 'three', '?', 'sent', '4', 'is', 'cool', '!', 'Now', 'it', \"'s\", 'your', 'turn', '.']\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Starting point : tokens\n",
        "\"\"\"\n",
        "tokens = tokenize_word_text(text1)\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "4c79dd12-26f2-47b6-b971-3ff1a683ff77",
        "_uuid": "42716b4b1ed4fff263d369be1e79936872a38997",
        "id": "v8yXJX7G2H2w"
      },
      "source": [
        "##CONVERTIMOS TODAS LAS LETRAS A UN SOLO FORMATO EN ESTE CASO MINUSCULA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "_cell_guid": "15fe3d4e-3f52-45fe-bb2e-b86f1004990e",
        "_uuid": "fc43848c271cc45ad38708a9c8be9b73e50655ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yAmBsHU02H2y",
        "outputId": "6c54bd86-e840-49e7-d3e9-4782b6b59b8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['this', \"'s\", 'ã', 'sent', 'tokenize', 'test', '.', 'this', \"'s\", 'sent', 'two', '.', 'is', 'this', 'sent', 'three', '?', 'sent', '4', 'is', 'cool', '!', 'now', 'it', \"'s\", 'your', 'turn', '.']\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Converting all letters to lower or upper case (common : lower case)\n",
        "\"\"\"\n",
        "def convert_letters(tokens, style = \"lower\"):\n",
        "    if (style == \"lower\"):\n",
        "        tokens = [token.lower() for token in tokens]\n",
        "    else :\n",
        "        tokens = [token.upper() for token in tokens]\n",
        "    return(tokens)\n",
        "tokens = convert_letters(tokens)\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "ad5fd54f-d969-4f09-8c10-e48c2ed3fbd3",
        "_uuid": "477bf8dcf5e9fbfe1ff5ebdc91421f41e2bed59d",
        "id": "zSWRg0PD2H20"
      },
      "source": [
        "##REMOVEMOS ESPACIOS EN BLANCO "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "_cell_guid": "2a9c2598-4349-412d-b88c-edf37b1153d3",
        "_uuid": "47ccca040a51bfbee26e9469a4eabd45b07c2b19",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jvq_7-_U2H23",
        "outputId": "7abba353-20ae-44d3-ac6b-019562213fa9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['this', \"'s\", 'ã', 'sent', 'tokenize', 'test', '.', 'this', \"'s\", 'sent', 'two', '.', 'is', 'this', 'sent', 'three', '?', 'sent', '4', 'is', 'cool', '!', 'now', 'it', \"'s\", 'your', 'turn', '.']\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Remove blancs on words\n",
        "\"\"\"\n",
        "def remove_blanc(tokens):\n",
        "    tokens = [token.strip() for token in tokens]\n",
        "    return(tokens)\n",
        "tokens = remove_blanc(tokens)\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "f1c3d964-cba1-4702-b43c-7b1c225ecb3f",
        "_uuid": "644dc4cc775f7614c447f3dbecec9a0f9e587b8c",
        "id": "n47-k8bU2H26"
      },
      "source": [
        "## REMOVEMOS LOS SIGNOS DE PUNTUACIÓN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "_cell_guid": "dd6b20d6-463c-4c9d-9516-2ffd33547d69",
        "_uuid": "30bee59027e5343b62ad3f70c71216109cbef382",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "37ADDLOo2H27",
        "outputId": "61cd4862-6f1f-449d-ab93-4054daed4d12"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'this s ã sent tokenize test this s sent two is this sent three sent 4 is cool now it s your turn'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "\"\"\"\n",
        "FUNCIÓN PARA REMOVER ANTES DE GENERAR LOS TOKENS \n",
        "\"\"\"\n",
        "def remove_before_token(sentence, keep_apostrophe = False):\n",
        "    sentence = sentence.strip()\n",
        "    if keep_apostrophe:\n",
        "        PATTERN = r'[?|$|&|*|%|@|(|)|~]'\n",
        "        filtered_sentence = re.sub(PATTERN, r' ', sentence)\n",
        "    else :\n",
        "        PATTERN = r'[^a-zA-Z0-9]'\n",
        "        filtered_sentence = re.sub(PATTERN, r' ', sentence)\n",
        "    return(filtered_sentence)\n",
        "remove_before_token(text1)\n",
        "\n",
        "\"\"\"\n",
        "FUNCIÓN PARA REMOVER DESPUES DE GENERAR LOS TOKENS  \n",
        "\"\"\"\n",
        "def remove_after_token(tokens): \n",
        "    pattern = re.compile('[{}]'.format(re.escape(string.punctuation))) \n",
        "    filtered_tokens = filter(None, [pattern.sub('', token) for token in tokens]) \n",
        "    filtered_text = ' '.join(filtered_tokens) \n",
        "    return filtered_text \n",
        "remove_after_token(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "1db8c6cd-b065-4886-9c0e-2b5df0a666ee",
        "_uuid": "06dd7a53a531c66fa012c1392e11aee524a59eae",
        "id": "EFy-NXxw2H3B"
      },
      "source": [
        "## Expanding Contraction \n",
        "\n",
        "_ Contraction _ are shortened version of words or syllables. They exist in either written or spoken forms. By nature, contraction pose a problem for NLP and text analysis because we have a special apostrophe character in the word. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "_cell_guid": "6253c435-5736-456f-8b77-c86608342b6a",
        "_uuid": "7ea49b418472e315d34af5fe7110f6020d9661b1",
        "collapsed": true,
        "id": "GDT6kBN02H3D"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Expanding contraction \n",
        "-----------------------------------------------------\n",
        "Generamos diccionario con las palabras recortadas y su correspondiente forma original\n",
        "\"\"\"\n",
        "\n",
        "CONTRACTION_MAP = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \n",
        "                   \"can't've\": \"cannot have\", \"'cause\": \"because\", \"could've\": \"could have\", \n",
        "                   \"couldn't\": \"could not\", \"couldn't've\": \"could not have\",\"didn't\": \"did not\", \n",
        "                   \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \n",
        "                   \"hadn't've\": \"had not have\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \n",
        "                   \"he'd\": \"he would\", \"he'd've\": \"he would have\", \"he'll\": \"he will\", \n",
        "                   \"he'll've\": \"he he will have\", \"he's\": \"he is\", \"how'd\": \"how did\", \n",
        "                   \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\", \n",
        "                   \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \n",
        "                   \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \n",
        "                   \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\", \n",
        "                   \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \n",
        "                   \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \n",
        "                   \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \n",
        "                   \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \n",
        "                   \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \n",
        "                   \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \n",
        "                   \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \n",
        "                   \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\",\n",
        "                   \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \n",
        "                   \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \n",
        "                   \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \n",
        "                   \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \n",
        "                   \"this's\": \"this is\",\n",
        "                   \"that'd\": \"that would\", \"that'd've\": \"that would have\",\"that's\": \"that is\", \n",
        "                   \"there'd\": \"there would\", \"there'd've\": \"there would have\",\"there's\": \"there is\", \n",
        "                   \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \n",
        "                   \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \n",
        "                   \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \n",
        "                   \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \n",
        "                   \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \n",
        "                   \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\", \n",
        "                   \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \n",
        "                   \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \n",
        "                   \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \n",
        "                   \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \n",
        "                   \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \n",
        "                   \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \n",
        "                   \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n",
        "                   \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
        "                   \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \n",
        "                   \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" } \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "_cell_guid": "5859c922-24d2-4565-a1ce-7f3ef6780398",
        "_uuid": "433670fa9155d25b075182189b5cca487c341409",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2AQN93d2H3L",
        "outputId": "dd750e44-9d30-4b31-98e3-5af556db1aff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text before expanding contraction : \n",
            "  ThIs's   ã sent tokenize test  .  this's sent two. is this sent three? sent 4 is cool! Now it's your turn.\n",
            "\n",
            " Text after expanding contraction : \n",
            "  ['This is   ã sent tokenize test  .', 'this is sent two.', 'is this sent three?', 'sent 4 is cool!', 'Now it is your turn.']\n"
          ]
        }
      ],
      "source": [
        "def expand_contractions(sentence, contraction_mapping): \n",
        "     \n",
        "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())),  \n",
        "                                      flags=re.IGNORECASE|re.DOTALL) \n",
        "    def expand_match(contraction): \n",
        "        match = contraction.group(0) \n",
        "        first_char = match[0] \n",
        "        expanded_contraction = contraction_mapping.get(match) if contraction_mapping.get(match) else contraction_mapping.get(match.lower())                        \n",
        "        expanded_contraction = first_char+expanded_contraction[1:] \n",
        "        return expanded_contraction \n",
        "         \n",
        "    expanded_sentence = contractions_pattern.sub(expand_match, sentence) \n",
        "    return expanded_sentence \n",
        "     \n",
        "expanded_corpus = [expand_contractions(txt, CONTRACTION_MAP)  \n",
        "                     for txt in sent_tokenize(text1)]     \n",
        "\n",
        "print (\"Text before expanding contraction : \\n \", text1)\n",
        "print (\"\\n Text after expanding contraction : \\n \",expanded_corpus) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "a81e9372-e347-41b0-a4ab-0501538c5340",
        "_uuid": "ec41ec55980d65f14746e074e745e09a79c9ddb7",
        "id": "aFTsAZUT2H3P"
      },
      "source": [
        "## Eliminación de acentos y otros signos diacríticos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "_cell_guid": "bd07e24e-3315-4a73-9a6c-5c9e9a363505",
        "_uuid": "50125a241d447a54823a4bced8a6ef089bade074",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hiz_1QtT2H3Q",
        "outputId": "4e17f11b-b03a-4b16-eb9d-122aa144ebe1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After removing accent markes before tokenize words : \n",
            " ThIs's   a sent tokenize test  .  this's sent two. is this sent three? sent 4 is cool! Now it's your turn.\n",
            "After removing accent markes  ['this', \"'s\", 'a', 'sent', 'tokenize', 'test', '.', 'this', \"'s\", 'sent', 'two', '.', 'is', 'this', 'sent', 'three', '?', 'sent', '4', 'is', 'cool', '!', 'now', 'it', \"'s\", 'your', 'turn', '.']\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Removing accent marks and other diacritics - before tokens words\n",
        "\"\"\"\n",
        "def remove_accent_before_tokens(sentences):\n",
        "    res = unidecode.unidecode(sentences)\n",
        "    return(res)\n",
        "tmp = remove_accent_before_tokens(text1)\n",
        "print(\"After removing accent markes before tokenize words : \\n\", tmp)\n",
        "\n",
        "\"\"\"\n",
        "Removing accent marks and other diacritics - After tokens words\n",
        "\"\"\"\n",
        "\n",
        "def remove_accent_after_tokens(tokens):\n",
        "    tokens = [unidecode.unidecode(token) for token in tokens]\n",
        "    return(tokens)\n",
        "tokens = remove_accent_after_tokens(tokens)\n",
        "print(\"After removing accent markes \", tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "e70ed6cc-debb-4840-973d-1586b0161930",
        "_uuid": "8fb63f67858a2397e1cf727d292e5b0eb91002f0",
        "id": "eRv-YDl42H3T"
      },
      "source": [
        "## STOPWORDS "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qO3lL1Sq-lD_",
        "outputId": "fcb03907-34e9-4812-955b-e99e207b65a6"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "_cell_guid": "452ad5e7-be77-4469-a1b9-d33a9083d73b",
        "_uuid": "3b3b049bea4af2a3049c0476e042fe2cdae948c2",
        "id": "4SfKDxZq2H3U"
      },
      "outputs": [],
      "source": [
        "#Mirar\n",
        "# \"\"\"\n",
        "# Use a stopwords list\n",
        "# \"\"\"\n",
        "# stopword_list =stopwords.words('english')\n",
        "# print(\"StopWords List in English : \\n\", stopword_list)\n",
        "\n",
        "#Create your own stopwords list\n",
        "stopwords = ['a', 'about', 'above', 'across', 'after', 'afterwards']\n",
        "stopwords += ['again', 'against', 'all', 'almost', 'alone', 'along']\n",
        "stopwords += ['this', 'is', 'your']\n",
        "\n",
        "\"\"\"\n",
        "Function StopWords\n",
        "\"\"\"\n",
        "def removeStopwords_after_tokens(wordlist, stopwords):\n",
        "    return [w for w in wordlist if w not in stopwords]\n",
        "\n",
        "# Given a list of words, remove any that are in a list of stop words.\n",
        "def removeStopwords_before_tokens(text, stopwords):\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    return [w for w in tokens if w not in stopwords]\n",
        "\n",
        "# Example of calling the function : \n",
        "# removeStopwords_before_tokens(tokens, stopwords)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = nltk.word_tokenize(text1)\n",
        "removeStopwords_after_tokens(tokens, stopwords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wn6D4Q-e_war",
        "outputId": "ee52cb61-5050-412d-d9e9-ea7d8ddf9d0b"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ThIs',\n",
              " \"'s\",\n",
              " 'ã',\n",
              " 'sent',\n",
              " 'tokenize',\n",
              " 'test',\n",
              " '.',\n",
              " \"'s\",\n",
              " 'sent',\n",
              " 'two',\n",
              " '.',\n",
              " 'sent',\n",
              " 'three',\n",
              " '?',\n",
              " 'sent',\n",
              " '4',\n",
              " 'cool',\n",
              " '!',\n",
              " 'Now',\n",
              " 'it',\n",
              " \"'s\",\n",
              " 'turn',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "d4a9bae6-39d6-4a9f-bb65-ed90f243cb56",
        "_uuid": "462d76c71a76f92854e385789bbedb1bab3d0f8e",
        "id": "3-Yms6OI2H3a"
      },
      "source": [
        "## CORRECCIÓN DE ERRORES GRAMATICALES **\n",
        "\n",
        "* _ Repeated characters  _ \n",
        "\n",
        "One of the main challengeces faced in text normalization is the presence of incorrect words in the text. The definition of incorrect here covers words that have spelling mistakes as well as words with several letters repeated[](http://)\n",
        "\n",
        "* _ Correcting spelling  _ \n",
        "\n",
        "Another problem : wrong spelling that occur due to human error or even machine based errors. \n",
        "Common use : Algo developed by Peter Norvig (Google)\n",
        "\n",
        "Description : http://norvig.com/spell-correct.html\n",
        "\n",
        "The main objectif is that given a word, we need to find the most likely word that is the correct form of that word. the approach we would follow is to generate a set of candidate words that are near to our input word and select the most likely word from this set as the correct word. \n",
        "\n",
        "Source pgm : https://github.com/dipanjanS/text-analytics-with-python/blob/master/Chapter-3/spelling_corrector.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "_cell_guid": "b3e80a78-44e1-4939-929e-3c79f315fbad",
        "_uuid": "a7418a25f6eeea4455b437f9681e7c9ddb820b00",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dp5mbCXX2H3c",
        "outputId": "f284d6c7-f987-4fe1-9303-33bd03f2ca6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]\n",
            "1161192\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "\"\"\"\n",
        "Method 1 : Using the brown corpus in NLTK and \"in\" operator\n",
        "\"\"\"\n",
        "from nltk.corpus import brown\n",
        "nltk.download('brown')\n",
        "word_list = brown.words()\n",
        "print(word_list)\n",
        "print(len(word_list))\n",
        "\n",
        "word_set = set(word_list)\n",
        "\"looked\" in word_set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ac35f686-9ecf-42ea-9ce9-e81ee1e3ce32",
        "_uuid": "ea24799e4015db6d7e46fb0e0c76631b8875b788",
        "id": "yEFM9xHN2H3f",
        "outputId": "859382f7-f8d1-4e05-df45-17ad920690c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "spelling\n",
            "final\n",
            "mistakes\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Method 2 : Peter Norvig sur un seul mot\n",
        "\"\"\"\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "from collections import Counter\n",
        "\n",
        "def words(text): return re.findall(r'\\w+', text.lower())\n",
        "WORDS = Counter(words(open('../input/big.txt').read()))\n",
        "\n",
        "def P(word, N=sum(WORDS.values())): \n",
        "    \"Probability of `word`.\"\n",
        "    return WORDS[word] / N\n",
        "\n",
        "def correction(word): \n",
        "    \"Most probable spelling correction for word.\"\n",
        "    return max(candidates(word), key=P)\n",
        "\n",
        "def candidates(word): \n",
        "    \"Generate possible spelling corrections for word.\"\n",
        "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
        "\n",
        "def known(words): \n",
        "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
        "    return set(w for w in words if w in WORDS)\n",
        "\n",
        "def edits1(word):\n",
        "    \"All edits that are one edit away from `word`.\"\n",
        "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
        "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
        "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
        "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
        "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
        "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
        "    return set(deletes + transposes + replaces + inserts)\n",
        "\n",
        "def edits2(word): \n",
        "    \"All edits that are two edits away from `word`.\"\n",
        "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
        "    \n",
        "\"\"\"\n",
        "Exemple avec des mots au hasard \n",
        "\"\"\"\n",
        "print(correction('speling'))\n",
        "print(correction('fial'))\n",
        "print(correction(\"misstkaes\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "39290ec5-02f9-4180-b39f-4b88da10a024",
        "_uuid": "6dea688497ce36b4e4d0d82336838af83d4348e0",
        "id": "mhiHTtpJ2H3i",
        "outputId": "5ca60746-5a8e-4e43-c586-bb062a40b5c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "his is a sent tokenize test with mistakes in spelling a this is sent two a is this sent three a sent 4 is cool a now it is your turn a\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Exemple avec notre text initial \n",
        "\"\"\"\n",
        "text1 = \"ThIs's   ã sent tokenize test  wiith miststakes in speling.  this's sent two. is this sent three? sent 4 is cool! Now it's your turn.\"\n",
        "\n",
        "def correct_word_in_sentence(text):\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    r = [correction(token) for token in tokens]\n",
        "    return (r)\n",
        "tmp = ' '.join(correct_word_in_sentence(text1))\n",
        "type(tmp)\n",
        "print(tmp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "889ce453-f50e-480e-a0f5-53991959aa25",
        "_uuid": "4d73b1f10085eac0261f37ac47994f2047b5ac7c",
        "id": "ONJv_viA2H3k"
      },
      "source": [
        "## CONVIRTIENDO NUMERO A TEXTO **"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "_cell_guid": "a777af8d-4dd2-4ec3-9456-0e109ebfa446",
        "_uuid": "52bd0776ddf00f1ad634e12dcb9e36c0f67babc2",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "PMI04ySh2H3l",
        "outputId": "0d75e308-7744-40a5-9a38-43e542984705"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nConverting numbers into words\\n\\n\\n\\n\\n                                                TO DO\\n\\n\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "\"\"\"\n",
        "Converting numbers into words\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                                               POR HACER\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "8a0c3e8f-2e87-4fd7-9f2f-946703de8bf1",
        "_uuid": "d244ba9441deec1863bed21d2601b47f8673059e",
        "id": "AYiCv8r52H3o"
      },
      "source": [
        "## TODO EL PROCESO RESUMIDO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9c19b959-eca7-46eb-9e01-d6385321c035",
        "_uuid": "54c54d7ac3470eecde9522dcb6c97d3f0aba337c",
        "id": "7h0JaF952H3p",
        "outputId": "553d78fb-725e-4326-e2c1-382c5333299c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hello sent tokenize test with mistakes in spelling sent two sent three sent 4 cool now it turn\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Driver\n",
        "\"\"\"\n",
        "\n",
        "text1 = \"Heloo ! ThIs's  ã sent tokenize test  wiith misstkaes in speling.  this's sent two. is this sent three? sent 4 is cool! Now it's your turn.\"\n",
        "\n",
        "def preprocessing(content):\n",
        "    sentences = tokenize.sent_tokenize(content)\n",
        "    cleaned_sentences = []\n",
        "    for s in sentences :\n",
        "        # 1- Lower case \n",
        "        s = s.lower()\n",
        "        \n",
        "        # 2- Supprimer les blancs :\n",
        "        s = s.strip()  \n",
        "        \"\"\"\n",
        "        A revoir\n",
        "        \"\"\"\n",
        "        # 3- Unicode - supprimer les accents\n",
        "        s = remove_accent_before_tokens(s)\n",
        "        \n",
        "        # 4- Expanding contraction\n",
        "        # Remarque : on fait l'expansion en même temps que l'on transforme la list en str\n",
        "        s = ''.join([expand_contractions(txt, CONTRACTION_MAP)\n",
        "                     for txt in sent_tokenize(s)]  )\n",
        "        \n",
        "        # 5- Remove punctuation ?\n",
        "        # A faire après avoir fait l'expanding contraction car sinon supprimer les ' qui symbolise la contraction\n",
        "        s = remove_before_token(s)\n",
        "\n",
        "        # 6- Correcting words\n",
        "        # s = ' '.join(correct_word_in_sentence(s))\n",
        "        s = ' '.join(correct_word_in_sentence(s))\n",
        "        \n",
        "        # 7- Remove StopWords\n",
        "        s = ' '.join(removeStopwords_before_tokens(s, stopwords))\n",
        "        \n",
        "        # 8- Remove Numbers or transform it in char\n",
        "        \"\"\"\n",
        "        TODO\n",
        "        \"\"\"\n",
        "        \n",
        "        # Enregistrement des resultats\n",
        "        cleaned_sentences.append(s)\n",
        "    return(cleaned_sentences) \n",
        "        \n",
        "\n",
        "result = ' '.join(preprocessing(text1))\n",
        "print(result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LIBRERIAS PARA CONSULTAR\n"
      ],
      "metadata": {
        "id": "7-sFQLkAC06H"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "bf7aa6f3-0732-483c-ab7b-e7aea4d4eadb",
        "_uuid": "89b5f9ddc5c5f184ded59627ab6be25e6df7c488",
        "id": "9F5I8Z3l2H3s"
      },
      "source": [
        "## II/ Lexicon Normalization\n",
        "\n",
        "### A/ Stemming\n",
        "Porter Stemmer Algo : https://tartarus.org/martin/PorterStemmer/\n",
        "\n",
        "Source : http://textminingonline.com/dive-into-nltk-part-iv-stemming-and-lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from nltk.stem import WordNetLemmatizer\n",
        "# from nltk.stem import PorterStemmer\n",
        "# from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "# from nltk.stem import SnowballStemmer\n",
        "# from nltk.stem.lancaster import LancasterStemmer\n",
        "\n"
      ],
      "metadata": {
        "id": "K2V4s3y9D7dC"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}